# 主要创新点

## MLA（Multi-Head Latent Attention）
先看下整体的计算过程，分为两部分。一部分是QK的计算，另一部分是V。
输入是隐藏层$h_t$，可以指最开始的输入，又或是上一层的输出。首先通过$c_{t}^{KV}=W^{DKV}h_t$，将d维的$h_t$压缩为$d_c$维。这里是做了一个低秩压缩。在V3中d=7168，d_c=512。
后续会通过W^{UK}、W^{UV}两个矩阵将c_{t}^{KV}的维度扩展为d。也就是：$k^{C}_{t}=W^{UK}c_{t}^{KV}$、$v^{C}_{t}=W^{UV}c_{t}^{KV}$。

经过上面的操作KV-cache就由原来的d * d，变成了2 * d * d_c。每当用到KV的时候就使用c_{t}^{KV}计算得到。

Q也是做了类似与KV的转换：$c_{t}^{Q}=W^{DQ}h_t$、$q^{C}_{t}=W^{UQ}c_{t}^{Q}$。V3中的d_q=1536

> 理解KV-cache：
> 进行推理的过程是一个自回归的过程，也就是从一个起始符号得到第一个字，通过起始符号和第一个字继续得到第二个字。后续不断重复。这样可以发现在推理第10个字的时候，前9个字已经计算过KV了。在推理第11个字的时候，还需要前9个字的KV。这个时候就可以把前面的KV缓存下来，只要计算最新那个字的KV，然后拼接到原来缓存的KV之后即可。
> 这个缓存的KV就是KV-cache。所以在推理的时候主要的计算就在注意力那边了。如果没有KV-cache每推理得到一个字都要重新对前面生成的内容计算KV

## Moe


## MTP(Multi-Token Prediction)

## FP8训练
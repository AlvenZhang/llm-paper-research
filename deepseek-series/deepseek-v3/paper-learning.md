
## Abstract：
V3是
1. 性能卓越的混合专家语言模型
2. 参数量是671B，每个token激活37B参数，实现了高效推理与高性价比训练。

V3采用了`MLA`与`DeepSeekMoE`两种架构，并且提出了一种`无辅助损失的负载均衡策略`，设定了`多token预测训练目标`

**训练过程：** 
1. 14.5万亿个多样、高质量token进行预训练
2. 然后继续通过监督微调和强化学习进一步挖掘能力。

V3完整训练过程仅需278.8万小时的H800GPU。训练过程中未遇到无法恢复的损失尖峰，也无需执行任何回滚操作

> ps：多token预测是指让模型根据前文预测后面的多个token，而不只是“下一个”

benchmark performance在多个数据集/评估效果上跟v2.5、Qwen2.5、Llama、GPT4o、Claude-3.5进行比较。
后续可以留意一下这些比较多数据集/评估效果分别是什么


## Intro
介绍了开源模型的发展，包括DeepSeek系列、Llama系列、Qwen系列、Mistra系列。
表示自己致力于模型高性能与成本经济性之间寻求平衡。DeepSeekV3架构采用MLA实现高效推理，使用MoE实现高性价比训练。还提出了无辅助损失的负载均衡策略和引入了多token预测的训练目标。
为了高效训练，使用了FP8混合精度，验证了低精度训练的有效性。设计了流水线并行算法实现了极高的训练效率。
ps：这里提到的流水线并行训练没看明白
回顾了一下训练阶段：预训练阶段、两个阶段的上下文扩展、后训练（监督微调、强化学习和从DeepSeekR1中蒸馏推理能力）
在基准测试评估中，V3已经成为当前最强的开源模型，并且与优秀的闭源模型相当
最后又强调了V3的训练低成本。
最后的最后总结了一下V3的核心贡献：
1. 架构层面：负载均衡策略与多token训练目标
2. 预训练层面：极致的训练效率
3. 后训练层面：从DeepSeekR1蒸馏知识
4. 核心评估结果：知识领域与代码数学推理领域，DeepSeekV3的表现

## Architecture
模型的结构主要是与DeepSeekV2相同的MLA和MoE架构，V2与V3的区别在于V3引入了无辅助损失的负载均衡策略，它可以缓解为确保负载均衡而对模型性能造成的损耗。
Architecture主要介绍了1. MLA、负载均衡策略；2. 多token预测
MLA：
核心是对注意力的K和V进行低秩联合压缩，减少推理过程中的KV缓存。
低秩联合压缩实现过程大致如下：隐藏层h与向量W^DKV相乘得到C^KV，C^KV分别与W^UK和W^UV相乘得到Key和Value。隐藏层先向下投影得到了c，后向上投影得到了K、V。具体的参数维度大小还需要结合代码去看一下。
Query的操作也是类似的。另外Q和V还要分别做旋转位置嵌入。
得到QKV之后，进行常规的注意力计算就得到了最终的输出
RoPE旋转位置嵌入没有明白。
DeepSeekMoE with Auxiliary-Loss_free Load Balancing
experts分为两部分，一部分是shared，另一部分是routed。每个shared对于输入都有效，部分routed对于输入有效。
过程：FFN输入经过相似度之后进行Sigmoid归一化；取相似度最高的K个其余赋值为0（也就是决定了哪些专家是工作的），对于每个相似度都进行归一化；将工作的routed、全部的shared与输入相加就得到了FFN的输出。
专家负载不均衡会导致路由崩溃，也就是频繁选择到某一个或几个专家。传统方式是通过辅助损失解决（也就是添加额外的损失函数，可以再了解一下这里），但是辅助损失会增加计算量，破坏性能。本文提出了为每个专家的亲和度分数后面添加一个偏置项。在训练过程中会监控专家的负载情况，如果某个专家过载就会将偏置项减去gamma，如果某个专家调用过少就加上gamma，gamma是一个超参数。（这里可以看一下，是不是每个专家都有一个对应的gamma）

序列级辅助损失：上文讲到的无辅助损失的负载均衡策略主要是作用于不同批次之间专家的负载均衡。还存在一个批次内部某个专家被频繁调用，导致专家超负荷的情况，于是使用了这种方式补充。

**总损失** 由每个专家的 “负载均衡度 × 平均被选概率” 求和得到，alpha确保该损失仅起 “微调作用”，不干扰模型核心能力学习

为了降低训练成本，每个token最多送给M个专家。
文中还提到，由于优秀的负载均衡策略。使得不会因为某个专家过于繁忙而丢弃token

Multi-Token Prediction


Infrastructures
设计了一种高效流水线并行的DualPipe算法，充分利用各种资源。完成V3的训练

Pre-Training


Post-Training

Conclusion, Limitations, and Future Directions
